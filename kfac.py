import torch.nn.functional as F
from torch.autograd import Variable
from torch.optim.optimizer import Optimizer


class KFAC(Optimizer):

    def __init__(self, net, eps, pi=False, update_freq=1, alpha=1.0):
        """ K-FAC Preconditionner for Linear and Conv2d layers.

        Computes the K-FAC of the second moment of the gradients.
        It works for Linear and Conv2d layers and silently skip other layers.

        Args:
            net (torch.nn.Module): Network to precondition.
            eps (float): Tikhonov regularization parameter for the inverses.
            pi (bool): Computes pi correction for Tikhonov regularization.
            update_freq (int): Perform inverses every update_freq updates.
            alpha (float): Running average parameter (if == 1, no r. ave.).

        """
        self.eps = eps
        self.pi = pi
        self.update_freq = update_freq
        self.alpha = alpha
        self.params = []
        self.a_mappings = {}
        self.g_mappings = {}
        self._iteration_counter = 0
        for mod in net.modules():
            mod_class = mod.__class__.__name__
            if mod_class in ['Linear', 'Conv2d']:
                mod.register_forward_pre_hook(self._save_input)
                mod.register_backward_hook(self._save_grad_output)
                params = [mod.weight]
                if mod.bias is not None:
                    params.append(mod.bias)
                d = {'params': params, 'mod': mod, 'layer_type': mod_class}
                if mod_class == 'Conv2d':
                    # Adding gathering filter for convolution
                    d['gathering_filter'] = self._get_gathering_filter(mod)
                self.params.append(d)
        super(KFAC, self).__init__(self.params, {})

    def step(self):
        """Performs one step of preconditioning."""
        for group in self.param_groups:
            # Getting parameters
            if len(group['params']) == 2:
                weight, bias = group['params']
            else:
                weight = group['params'][0]
                bias = None
            state = self.state[weight]
            # Update convariances and inverses
            if self._iteration_counter % self.update_freq == 0:
                self._compute_covs(group, state)
                ixxt, iggt = self._inv_covs(state['xxt'], state['ggt'])
                state['ixxt'] = ixxt
                state['iggt'] = iggt
            else:
                if self.alpha != 1:
                    self._compute_covs(group, state)
            # Preconditionning
            self._precond(weight, bias, group, state)
        self._iteration_counter += 1

    def _save_input(self, mod, i):
        """Saves input of layer to compute covariance."""
        self.a_mappings[mod] = i[0]

    def _save_grad_output(self, mod, grad_input, grad_output):
        """Saves grad on output of layer to compute covariance."""
        self.g_mappings[mod] = grad_output[0] * grad_output[0].size(0)

    def _precond(self, weight, bias, group, state):
        """Applies preconditioning."""
        ixxt = state['ixxt']
        iggt = state['iggt']
        g = weight.grad.data
        s = g.shape
        if group['layer_type'] == 'Conv2d':
            g = g.contiguous().view(s[0], s[1]*s[2]*s[3])
        if bias is not None:
            gb = bias.grad.data
            g = torch.cat([g, gb.view(gb.shape[0], 1)], dim=1)
        g = torch.mm(torch.mm(iggt, g), ixxt)
        if bias is not None:
            gb = g[:, -1].contiguous().view(*bias.shape)
            bias.grad.data = gb
            g = g[:, :-1]
        g = g.contiguous().view(*s)
        weight.grad.data = g

    def _compute_covs(self, group, state):
        """Computes the covariances."""
        mod = group['mod']
        x = self.a_mappings[group['mod']]
        bs = x.shape[0]
        gy = self.g_mappings[group['mod']]
        # Computation of xxt
        if group['layer_type'] == 'Conv2d':
            x = F.conv2d(x, group['gathering_filter'],
                         stride=mod.stride, padding=mod.padding,
                         groups=mod.in_channels)
            x = x.data.permute(1, 0, 2, 3).contiguous().view(x.shape[1], -1)
        else:
            x = x.data.t()
        if mod.bias is not None:
            ones = torch.ones_like(x[:1])
            x = torch.cat([x, ones], dim=0)
        if self._iteration_counter == 0:
            state['xxt'] = torch.mm(x, x.t()) / float(x.shape[1])
        else:
            state['xxt'].addmm_(mat1=x, mat2=x.t(),
                                beta=(1. - self.alpha),
                                alpha=self.alpha / float(x.shape[1]))
        # Computation of ggt
        if group['layer_type'] == 'Conv2d':
            gy = gy.data.permute(1, 0, 2, 3)
            gy = gy.contiguous().view(gy.shape[0], -1)
        else:
            gy = gy.data.t()
        if self._iteration_counter == 0:
            state['ggt'] = torch.mm(gy, gy.t()) / float(bs)
        else:
            state['ggt'].addmm_(mat1=gy, mat2=gy.t(),
                                beta=(1. - self.alpha),
                                alpha=self.alpha / float(bs))

    def _inv_covs(self, xxt, ggt):
        """Inverses the covariances."""
        # Computes pi
        pi = 1.0
        if self.pi:
            tx = torch.trace(xxt) / float(xxt.shape[0])
            tg = torch.trace(ggt) / float(ggt.shape[0])
            pi = (tx / tg) ** 0.5
        # Regularizes and inverse
        diag_xxt = xxt.new(xxt.shape[0]).fill_(self.eps * pi)
        diag_ggt = ggt.new(ggt.shape[0]).fill_(self.eps / pi)
        ixxt = (xxt + torch.diag(diag_xxt)).inverse()
        iggt = (ggt + torch.diag(diag_ggt)).inverse()
#         ixxt = (xxt).pinverse(rcond=1e-3)
#         iggt = (ggt).pinverse(rcond=1e-3)
        print(ixxt.size())
        return ixxt, iggt

    def _get_gathering_filter(self, mod):
        """Convolution filter that extract input patches."""
        kw, kh = mod.kernel_size
        g_filter = mod.weight.data.new(kw * kh * mod.in_channels, 1, kw, kh)
        g_filter.fill_(0)
        for i in range(mod.in_channels):
            for j in range(kw):
                for k in range(kh):
                    g_filter[k + kh*j + kw*kh*i, 0, j, k] = 1
        g_filter = Variable(g_filter, requires_grad=False)
        return g_filter